{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN / RNN / CNN-RNN Hybrid Implementations for human activity recognition\n",
    "\n",
    "This noteboook was used to compare the effectiveness of using a CNN, an RNNs and a CNN-RNN hybrid aproach for human activity recognition. This implementation uses tensorflow and tensorflow lite. An experimental build of tensorflow(tf-nightly) was used for this research to support LTSM cells on smartphones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# This is important!\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_CONTROL_FLOW_V2\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, TimeDistributed, Flatten, Lambda\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "The researchers opted use and already existing data set from [[1]](#References)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_experiments(labels_path):\n",
    "    \n",
    "    exp_path = 'HAPT Data Set/RawData/'\n",
    "    \n",
    "    exp_df = read_labels(labels_path)\n",
    "    acc_data_list_x = []\n",
    "    acc_data_list_y = []\n",
    "    acc_data_list_z = []\n",
    "\n",
    "    gyro_data_list_x = []\n",
    "    gyro_data_list_y = []\n",
    "    gyro_data_list_z = []\n",
    "\n",
    "    \n",
    "    for index, row in exp_df.iterrows():\n",
    "        exp_id = format(row['experiment ID'],'02d')\n",
    "        user_id = format(row['user ID'],'02d')\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        \n",
    "        acc_exp_path = exp_path + 'acc_exp' + exp_id + '_user' + user_id + '.txt'\n",
    "        gyro_exp_path = exp_path + 'gyro_exp' + exp_id + '_user' + user_id + '.txt'\n",
    "        \n",
    "        acc_data = pd.read_csv(acc_exp_path, delimiter = ' ', header = None, names = ['x','y','z'])\n",
    "        gyro_data = pd.read_csv(gyro_exp_path, delimiter = ' ', header = None, names = ['x','y','z'])\n",
    "        \n",
    "        acc_data_list_x.append(acc_data['x'][start:end].values)\n",
    "        acc_data_list_y.append(acc_data['y'][start:end].values)\n",
    "        acc_data_list_z.append(acc_data['z'][start:end].values)\n",
    "        \n",
    "        gyro_data_list_x.append(gyro_data['x'][start:end].values)\n",
    "        gyro_data_list_y.append(gyro_data['y'][start:end].values)\n",
    "        gyro_data_list_z.append(gyro_data['z'][start:end].values)\n",
    "        \n",
    "    exp_df['acc x'] = acc_data_list_x\n",
    "    exp_df['acc y'] = acc_data_list_y\n",
    "    exp_df['acc z'] = acc_data_list_z\n",
    "    exp_df['gyro x'] = gyro_data_list_x\n",
    "    exp_df['gyro y'] = gyro_data_list_y\n",
    "    exp_df['gyro z'] = gyro_data_list_z\n",
    "    \n",
    "    return exp_df\n",
    "\n",
    "def read_labels(file_path):\n",
    "\n",
    "    column_names = [\n",
    "        'experiment ID',\n",
    "        'user ID',\n",
    "        'activity ID',\n",
    "        'start',\n",
    "        'end'\n",
    "\n",
    "    ]\n",
    "\n",
    "    labels_df = pd.read_csv(file_path, delimiter=' ', header=None, names=column_names);\n",
    "\n",
    "    return labels_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "The time series data for each activity was segmented into 'windows' where each window was used as a sample together with its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_experiments(exp_df,segment_length,step): \n",
    "    #for x,y,z (acc) and x,y,z gyro\n",
    "    NUM_FEATURES = 3\n",
    "    features = ['acc x','acc y', 'acc z', 'gyro x', 'gyro y', 'gyro z']\n",
    "    \n",
    "    segments = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in exp_df.iterrows():\n",
    "        for i in range(0 , row['end'] - row['start'], step):\n",
    "            xa = row['acc x'][i : i + segment_length]\n",
    "            ya = row['acc y'][i : i + segment_length]\n",
    "            za = row['acc z'][i : i + segment_length]\n",
    "#             xg = row['gyro x'][i : i + segment_length]\n",
    "#             yg = row['gyro y'][i : i + segment_length]\n",
    "#             zg = row['gyro z'][i : i + segment_length]\n",
    "            \n",
    "            if len(xa) == segment_length:\n",
    "                segments.append([xa, ya, za\n",
    "#                                 xg, yg, zg\n",
    "                                ])\n",
    "                labels.append(row['activity ID'])\n",
    "\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, segment_length, NUM_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data\n",
    "The researchers opted for a window size of 100 without any overlaps. Furthermore, the data was split into 70% training and 30% testing samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'HAPT Data Set/RawData/labels.txt'\n",
    "exp_data = read_experiments(labels_path)\n",
    "\n",
    "SEGMENT_LENGTH = 100\n",
    "STEP = 100\n",
    "activity_labels_path = 'HAPT Data Set/activity_labels.txt' \n",
    "LABELS = pd.read_csv(activity_labels_path, delimiter = ' ', header = None)[1].tolist()\n",
    "\n",
    "segments,labels = segment_experiments(exp_data,SEGMENT_LENGTH,STEP)\n",
    "\n",
    "divider = int(len(segments) * .7)\n",
    "\n",
    "x_train = segments[:divider]\n",
    "y_train = labels[:divider]\n",
    "\n",
    "x_test = segments[divider:]\n",
    "y_test = labels[divider:]\n",
    "\n",
    "num_time_periods, dims = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = len(LABELS)\n",
    "\n",
    "input_shape = (num_time_periods * dims)\n",
    "x_train = x_train.reshape(x_train.shape[0],input_shape)\n",
    "\n",
    "y_train = y_train - 1\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "\n",
    "\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# test data\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],input_shape)\n",
    "\n",
    "y_test = y_test - 1\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "All models where trained with the same parameters - A batch size of 400 samples, on 50 Epochs, using the Adam optimizer and categorical cross entropy for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # The EarlyStopping callback monitors training accuracy:\n",
    "    # if it fails to improve for two consecutive epochs,\n",
    "    # training stops early\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='cnn_best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "            monitor='val_loss', save_best_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='acc', patience=50),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='tfb_logs', histogram_freq=0,\n",
    "              write_graph=True, write_images=True),\n",
    "    ]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    BATCH_SIZE = 400\n",
    "    EPOCHS = 50\n",
    "\n",
    "    # Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "    history = model.fit(x_train,\n",
    "                          y_train,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          callbacks=callbacks_list,\n",
    "                          validation_split=0.2,\n",
    "                          verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFLite for RNNs\n",
    "As of the moment, TFLite(which is used for run Tensorflow models on mobile) does not support LSTMs. To address this the researchers use the experimental build of the Tensorflow. The following function creates LSTM layers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "    \"\"\"Build the lstm layer.\n",
    "\n",
    "    Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.t\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "    \"\"\"\n",
    "    lstm_cells = []\n",
    "    for i in range(num_layers):\n",
    "        lstm_cells.append(\n",
    "            tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "                num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "    lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "    # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "    # to transpose to be time-majored.\n",
    "    transposed_inputs = tf.transpose(inputs, perm=[1, 0, 2])\n",
    "    outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "        lstm_layers,\n",
    "        transposed_inputs,\n",
    "        dtype='float32',\n",
    "        time_major = True)\n",
    "    unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "    return unstacked_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN MODEL\n",
    "The first model that was unsed in this research was a CNN model with the following architecture:\n",
    "![CNN Model](../images/cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add( Reshape( (SEGMENT_LENGTH, dims), input_shape=(input_shape,), name='cnn_input' ) )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu', input_shape = (SEGMENT_LENGTH,dims)) )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu') )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu') )\n",
    "cnn_model.add( MaxPooling1D(3) )\n",
    "cnn_model.add( Dropout(0.5) )\n",
    "cnn_model.add( Conv1D(160, 10, activation='relu') )\n",
    "cnn_model.add( Conv1D(160, 10, activation='relu') )\n",
    "cnn_model.add( GlobalAveragePooling1D() )\n",
    "cnn_model.add( Dense(num_classes, activation = tf.nn.softmax ,name='cnn_output') )\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "The RNN model uses LSTM cells and follows the following high level architectural design:\n",
    "![RNN Model](../images/rnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_model = tf.keras.Sequential()\n",
    "rnn_model.add( Reshape((SEGMENT_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "rnn_model.add( Input(shape=(SEGMENT_LENGTH, dims) ) )\n",
    "rnn_model.add( Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 64}))\n",
    "rnn_model.add( Dropout(0.5) )\n",
    "rnn_model.add( Dense(100, activation = 'relu'))\n",
    "rnn_model.add( Dense(num_classes, activation = 'softmax') )\n",
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-RNN Hybrid\n",
    "The hybrid model is a combination of multiple time distributed CNN layers and RNN layers using LSTMs. The model follows the following high level architectural design:\n",
    "![RNN Model](../images/cnn_rnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUB_STEPS = 5\n",
    "SUB_LENGTH = 20\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "cnn_rnn_model = tf.keras.Sequential()\n",
    "# cnn_rnn_model.add( Reshape((SEGMENT_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "cnn_rnn_model.add( Reshape((SUB_STEPS,SUB_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "cnn_rnn_model.add( TimeDistributed(Conv1D(filters = 100, kernel_size = 3,  activation='relu'), input_shape = (None, SUB_LENGTH, dims)))\n",
    "cnn_rnn_model.add( TimeDistributed(Conv1D(filters = 100, kernel_size = 3,  activation='relu')) )\n",
    "cnn_rnn_model.add( TimeDistributed(Dropout(0.5)) )\n",
    "cnn_rnn_model.add( TimeDistributed(MaxPooling1D(pool_size=2)) )\n",
    "cnn_rnn_model.add( (TimeDistributed(Flatten())) )\n",
    "cnn_rnn_model.add( Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 100}) )\n",
    "cnn_rnn_model.add( Dropout(0.5) )\n",
    "cnn_rnn_model.add( Dense(100, activation = 'relu'))\n",
    "cnn_rnn_model.add( Dense(num_classes, activation = 'softmax') )\n",
    "print(cnn_rnn_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments\n",
    "The researchers ran experiments wherein each model was retrained 10 times for each test scenario (accelerometer only vs accelerometer + gyroscope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CNN_2.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=path)\n",
    "interpreter.get_input_details()\n",
    "\n",
    "x_test = np.array([np.genfromtxt('test.txt',delimiter = ',')]).astype('float32')\n",
    "\n",
    "try:\n",
    "    interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "    assert False\n",
    "\n",
    "MINI_BATCH_SIZE = 1\n",
    "correct_case = 0\n",
    "for i in range(len(x_test)):\n",
    "    input_index = (interpreter.get_input_details()[0]['index'])\n",
    "    interpreter.set_tensor(input_index, x_test[i * MINI_BATCH_SIZE: (i + 1) * MINI_BATCH_SIZE])\n",
    "    interpreter.invoke()\n",
    "    output_index = (interpreter.get_output_details()[0]['index'])\n",
    "    result = interpreter.get_tensor(output_index)\n",
    "    # Reset all variables so it will not pollute other inferences.\n",
    "    interpreter.reset_all_variables()\n",
    "    # Evaluate.\n",
    "    print(result)\n",
    "    prediction = np.argmax(result)\n",
    "    if prediction == np.argmax(y_test[i]):\n",
    "        correct_case += 1\n",
    "\n",
    "print('TensorFlow Lite Evaluation result is {}'.format(correct_case * 1.0 / len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Jorge-L. Reyes-Ortiz, Luca Oneto, Albert Sam√†, Xavier Parra, Davide Anguita. Transition-Aware Human Activity Recognition Using Smartphones. Neurocomputing. Springer 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
