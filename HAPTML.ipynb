{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# This is important!\n",
    "# import os\n",
    "# os.environ[\"TF_ENABLE_CONTROL_FLOW_V2\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, TimeDistributed, Flatten, Lambda\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(file_path):\n",
    "\n",
    "    column_names = [\n",
    "        'experiment ID',\n",
    "        'user ID',\n",
    "        'activity ID',\n",
    "        'start',\n",
    "        'end'\n",
    "\n",
    "    ]\n",
    "\n",
    "    labels_df = pd.read_csv(file_path, delimiter=' ', header=None, names=column_names);\n",
    "\n",
    "    return labels_df\n",
    "\n",
    "def read_experiments(labels_path):\n",
    "    \n",
    "    exp_path = 'HAPT Data Set/RawData/'\n",
    "    \n",
    "    exp_df = read_labels(labels_path)\n",
    "    acc_data_list_x = []\n",
    "    acc_data_list_y = []\n",
    "    acc_data_list_z = []\n",
    "\n",
    "    gyro_data_list_x = []\n",
    "    gyro_data_list_y = []\n",
    "    gyro_data_list_z = []\n",
    "\n",
    "    \n",
    "    for index, row in exp_df.iterrows():\n",
    "        exp_id = format(row['experiment ID'],'02d')\n",
    "        user_id = format(row['user ID'],'02d')\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        \n",
    "        acc_exp_path = exp_path + 'acc_exp' + exp_id + '_user' + user_id + '.txt'\n",
    "        gyro_exp_path = exp_path + 'gyro_exp' + exp_id + '_user' + user_id + '.txt'\n",
    "        \n",
    "        acc_data = pd.read_csv(acc_exp_path, delimiter = ' ', header = None, names = ['x','y','z'])\n",
    "        gyro_data = pd.read_csv(gyro_exp_path, delimiter = ' ', header = None, names = ['x','y','z'])\n",
    "        \n",
    "        acc_data_list_x.append(acc_data['x'][start:end].values)\n",
    "        acc_data_list_y.append(acc_data['y'][start:end].values)\n",
    "        acc_data_list_z.append(acc_data['z'][start:end].values)\n",
    "        \n",
    "        gyro_data_list_x.append(gyro_data['x'][start:end].values)\n",
    "        gyro_data_list_y.append(gyro_data['y'][start:end].values)\n",
    "        gyro_data_list_z.append(gyro_data['z'][start:end].values)\n",
    "        \n",
    "    exp_df['acc x'] = acc_data_list_x\n",
    "    exp_df['acc y'] = acc_data_list_y\n",
    "    exp_df['acc z'] = acc_data_list_z\n",
    "    exp_df['gyro x'] = gyro_data_list_x\n",
    "    exp_df['gyro y'] = gyro_data_list_y\n",
    "    exp_df['gyro z'] = gyro_data_list_z\n",
    "    \n",
    "    return exp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_path = 'HAPT Data Set/RawData/labels.txt'\n",
    "exp_data = read_experiments(labels_path)\n",
    "exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_experiments(exp_df,segment_length,step): \n",
    "    #for x,y,z (acc) and x,y,z gyro\n",
    "    NUM_FEATURES = 3\n",
    "    features = ['acc x','acc y', 'acc z', 'gyro x', 'gyro y', 'gyro z']\n",
    "    \n",
    "    segments = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in exp_df.iterrows():\n",
    "        for i in range(0 , row['end'] - row['start'], step):\n",
    "            xa = row['acc x'][i : i + segment_length]\n",
    "            ya = row['acc y'][i : i + segment_length]\n",
    "            za = row['acc z'][i : i + segment_length]\n",
    "#             xg = row['gyro x'][i : i + segment_length]\n",
    "#             yg = row['gyro y'][i : i + segment_length]\n",
    "#             zg = row['gyro z'][i : i + segment_length]\n",
    "            \n",
    "            if len(xa) == segment_length:\n",
    "                segments.append([xa, ya, za])\n",
    "                labels.append(row['activity ID'])\n",
    "\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, segment_length, NUM_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_LENGTH = 100\n",
    "STEP = 100\n",
    "activity_labels_path = 'HAPT Data Set/activity_labels.txt' \n",
    "LABELS = pd.read_csv(activity_labels_path, delimiter = ' ', header = None)[1].tolist()\n",
    "\n",
    "segments,labels = segment_experiments(exp_data,SEGMENT_LENGTH,STEP)\n",
    "\n",
    "divider = int(len(segments) * .7)\n",
    "\n",
    "x_train = segments[:divider]\n",
    "y_train = labels[:divider]\n",
    "\n",
    "x_test = segments[divider:]\n",
    "y_test = labels[divider:]\n",
    "\n",
    "num_time_periods, dims = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = len(LABELS)\n",
    "\n",
    "input_shape = (num_time_periods * dims)\n",
    "x_train = x_train.reshape(x_train.shape[0],input_shape)\n",
    "\n",
    "y_train = y_train - 1\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "\n",
    "\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# test data\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],input_shape)\n",
    "\n",
    "y_test = y_test - 1\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cnn_input (Reshape)          (None, 100, 3)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 91, 100)           3100      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 82, 100)           100100    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 73, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 160)           160160    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 6, 160)            256160    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "cnn_output (Dense)           (None, 12)                1932      \n",
      "=================================================================\n",
      "Total params: 621,552\n",
      "Trainable params: 621,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add( Reshape( (SEGMENT_LENGTH, dims), input_shape=(input_shape,), name='cnn_input' ) )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu', input_shape = (SEGMENT_LENGTH,dims)) )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu') )\n",
    "cnn_model.add( Conv1D(100, 10, activation='relu') )\n",
    "cnn_model.add( MaxPooling1D(3) )\n",
    "cnn_model.add( Conv1D(160, 10, activation='relu') )\n",
    "cnn_model.add( Conv1D(160, 10, activation='relu') )\n",
    "cnn_model.add( GlobalAveragePooling1D() )\n",
    "cnn_model.add( Dropout(0.5) )\n",
    "cnn_model.add( Dense(num_classes, activation = tf.nn.softmax ,name='cnn_output') )\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.save('cnn_acconly_untrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "    \"\"\"Build the lstm layer.\n",
    "\n",
    "    Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "    \"\"\"\n",
    "    lstm_cells = []\n",
    "    for i in range(num_layers):\n",
    "        lstm_cells.append(\n",
    "            tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "                num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "    lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "    # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "    # to transpose to be time-majored.\n",
    "    transposed_inputs = tf.transpose(inputs, perm=[1, 0, 2])\n",
    "    outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "        lstm_layers,\n",
    "        transposed_inputs,\n",
    "        dtype='float32')\n",
    "    unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "    return unstacked_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0520 14:03:14.162262 21436 deprecation.py:506] From c:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 100, 3)            0         \n",
      "_________________________________________________________________\n",
      "input_1 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               122000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 133,312\n",
      "Trainable params: 133,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_model = tf.keras.Sequential()\n",
    "rnn_model.add( Reshape((SEGMENT_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "rnn_model.add( Input(shape=(SEGMENT_LENGTH, dims) ) )\n",
    "rnn_model.add( Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 100}))\n",
    "rnn_model.add( Dropout(0.5) )\n",
    "rnn_model.add( Dense(100, activation = 'relu'))\n",
    "rnn_model.add( Dense(num_classes, activation = 'softmax') )\n",
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save(\"rnn_acconly_untrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-RNN Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 5, 20, 3)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 18, 100)        1000      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 16, 100)        30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 16, 100)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 8, 100)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 800)            0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               440800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 483,212\n",
      "Trainable params: 483,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "SUB_STEPS = 5\n",
    "SUB_LENGTH = 20\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "cnn_rnn_model = tf.keras.Sequential()\n",
    "# cnn_rnn_model.add( Reshape((SEGMENT_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "cnn_rnn_model.add( Reshape((SUB_STEPS,SUB_LENGTH, dims), input_shape = (input_shape,)) )\n",
    "cnn_rnn_model.add( TimeDistributed(Conv1D(filters = 100, kernel_size = 3,  activation='relu'), input_shape = (None, SUB_LENGTH, dims)))\n",
    "cnn_rnn_model.add( TimeDistributed(Conv1D(filters = 100, kernel_size = 3,  activation='relu')) )\n",
    "cnn_rnn_model.add( TimeDistributed(Dropout(0.5)) )\n",
    "cnn_rnn_model.add( TimeDistributed(MaxPooling1D(pool_size=2)) )\n",
    "cnn_rnn_model.add( (TimeDistributed(Flatten())) )\n",
    "cnn_rnn_model.add( Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 100}) )\n",
    "cnn_rnn_model.add( Dropout(0.5) )\n",
    "cnn_rnn_model.add( Dense(100, activation = 'relu'))\n",
    "cnn_rnn_model.add( Dense(num_classes, activation = 'softmax') )\n",
    "print(cnn_rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model.save('cnn_rnn_acconly_untrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochEndCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print(K.eval(lr_with_decay))\n",
    "\n",
    "def train(model):\n",
    "    # The EarlyStopping callback monitors training accuracy:\n",
    "    # if it fails to improve for two consecutive epochs,\n",
    "    # training stops early\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='cnn_best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "            monitor='val_loss', save_best_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='acc', patience=50),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='tfb_logs', histogram_freq=0,\n",
    "              write_graph=True, write_images=True),\n",
    "#         EpochEndCallback\n",
    "    ]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    BATCH_SIZE = 400\n",
    "    EPOCHS = 100\n",
    "\n",
    "    # Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "    history = model.fit(x_train,\n",
    "                          y_train,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          callbacks=callbacks_list,\n",
    "                          validation_split=0.2,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4228 samples, validate on 1057 samples\n",
      "Epoch 1/50\n",
      "4228/4228 [==============================] - 9s 2ms/sample - loss: 1.6278 - acc: 0.4756 - val_loss: 1.0839 - val_acc: 0.6452\n",
      "Epoch 2/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 1.0251 - acc: 0.6518 - val_loss: 0.7026 - val_acc: 0.7644\n",
      "Epoch 3/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.7587 - acc: 0.7327 - val_loss: 0.5561 - val_acc: 0.8079\n",
      "Epoch 4/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.6680 - acc: 0.7588 - val_loss: 0.5653 - val_acc: 0.8070\n",
      "Epoch 5/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.6013 - acc: 0.7826 - val_loss: 0.4971 - val_acc: 0.8297\n",
      "Epoch 6/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.5668 - acc: 0.7938 - val_loss: 0.5503 - val_acc: 0.8297\n",
      "Epoch 7/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.5000 - acc: 0.8172 - val_loss: 0.4269 - val_acc: 0.8836\n",
      "Epoch 8/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.4824 - acc: 0.8214 - val_loss: 0.5533 - val_acc: 0.8221\n",
      "Epoch 9/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.4706 - acc: 0.8200 - val_loss: 0.4051 - val_acc: 0.8439\n",
      "Epoch 10/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.4171 - acc: 0.8446 - val_loss: 0.4251 - val_acc: 0.8704\n",
      "Epoch 11/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.3746 - acc: 0.8692 - val_loss: 0.3438 - val_acc: 0.8893\n",
      "Epoch 12/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.3505 - acc: 0.8706 - val_loss: 0.3912 - val_acc: 0.8836\n",
      "Epoch 13/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.3322 - acc: 0.8723 - val_loss: 0.3797 - val_acc: 0.9082\n",
      "Epoch 14/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.3231 - acc: 0.8808 - val_loss: 0.3197 - val_acc: 0.9111\n",
      "Epoch 15/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2937 - acc: 0.8874 - val_loss: 0.3510 - val_acc: 0.8931\n",
      "Epoch 16/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.2984 - acc: 0.8862 - val_loss: 0.3943 - val_acc: 0.8628\n",
      "Epoch 17/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2901 - acc: 0.8877 - val_loss: 0.3212 - val_acc: 0.9016\n",
      "Epoch 18/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2771 - acc: 0.8952 - val_loss: 0.3276 - val_acc: 0.8978\n",
      "Epoch 19/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.2642 - acc: 0.8983 - val_loss: 0.3760 - val_acc: 0.8950\n",
      "Epoch 20/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.2570 - acc: 0.8997 - val_loss: 0.3557 - val_acc: 0.9026\n",
      "Epoch 21/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2451 - acc: 0.9018 - val_loss: 0.3582 - val_acc: 0.9111\n",
      "Epoch 22/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2151 - acc: 0.9172 - val_loss: 0.2784 - val_acc: 0.9158\n",
      "Epoch 23/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.2074 - acc: 0.9177 - val_loss: 0.3561 - val_acc: 0.9101\n",
      "Epoch 24/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.2046 - acc: 0.9182 - val_loss: 0.4263 - val_acc: 0.8921\n",
      "Epoch 25/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.2127 - acc: 0.9158 - val_loss: 0.2845 - val_acc: 0.9224\n",
      "Epoch 26/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1884 - acc: 0.9290 - val_loss: 0.3336 - val_acc: 0.9234\n",
      "Epoch 27/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1596 - acc: 0.9331 - val_loss: 0.3315 - val_acc: 0.9215\n",
      "Epoch 28/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1595 - acc: 0.9357 - val_loss: 0.2283 - val_acc: 0.9376\n",
      "Epoch 29/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1504 - acc: 0.9361 - val_loss: 0.2638 - val_acc: 0.9328\n",
      "Epoch 30/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1459 - acc: 0.9416 - val_loss: 0.3650 - val_acc: 0.9177\n",
      "Epoch 31/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1448 - acc: 0.9357 - val_loss: 0.3447 - val_acc: 0.9177\n",
      "Epoch 32/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1615 - acc: 0.9385 - val_loss: 0.3091 - val_acc: 0.9253\n",
      "Epoch 33/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1452 - acc: 0.9416 - val_loss: 0.2555 - val_acc: 0.9385\n",
      "Epoch 34/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1218 - acc: 0.9475 - val_loss: 0.2737 - val_acc: 0.9319\n",
      "Epoch 35/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1153 - acc: 0.9567 - val_loss: 0.3164 - val_acc: 0.9234\n",
      "Epoch 36/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1174 - acc: 0.9527 - val_loss: 0.3011 - val_acc: 0.9262\n",
      "Epoch 37/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1489 - acc: 0.9428 - val_loss: 0.3630 - val_acc: 0.9167\n",
      "Epoch 38/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1566 - acc: 0.9409 - val_loss: 0.3901 - val_acc: 0.9158\n",
      "Epoch 39/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1361 - acc: 0.9475 - val_loss: 0.3397 - val_acc: 0.9177\n",
      "Epoch 40/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.1215 - acc: 0.9491 - val_loss: 0.3482 - val_acc: 0.9167\n",
      "Epoch 41/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.1082 - acc: 0.9551 - val_loss: 0.2737 - val_acc: 0.9272\n",
      "Epoch 42/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0984 - acc: 0.9612 - val_loss: 0.3407 - val_acc: 0.9319\n",
      "Epoch 43/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0994 - acc: 0.9607 - val_loss: 0.2785 - val_acc: 0.9451\n",
      "Epoch 44/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0954 - acc: 0.9636 - val_loss: 0.3258 - val_acc: 0.9300\n",
      "Epoch 45/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0929 - acc: 0.9596 - val_loss: 0.2746 - val_acc: 0.9432\n",
      "Epoch 46/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0897 - acc: 0.9669 - val_loss: 0.3762 - val_acc: 0.9177\n",
      "Epoch 47/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0871 - acc: 0.9643 - val_loss: 0.3621 - val_acc: 0.9234\n",
      "Epoch 48/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0740 - acc: 0.9735 - val_loss: 0.3117 - val_acc: 0.9300\n",
      "Epoch 49/50\n",
      "4228/4228 [==============================] - 6s 1ms/sample - loss: 0.0681 - acc: 0.9766 - val_loss: 0.3742 - val_acc: 0.9262\n",
      "Epoch 50/50\n",
      "4228/4228 [==============================] - 5s 1ms/sample - loss: 0.0676 - acc: 0.9754 - val_loss: 0.2966 - val_acc: 0.9404\n"
     ]
    }
   ],
   "source": [
    "train(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-850e431a589e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nAccuracy on test data: %0.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nLoss on test data: %0.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "score = rnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Train and Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 14:05:31.397998 21436 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4228 samples, validate on 1057 samples\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Node 'training/Adam/gradients/dropout_2_2/cond_grad/If': Connecting to invalid output 3 of source node dropout_2_2/cond which has 1 outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Node 'training/Adam/gradients/dropout_2_2/cond_grad/If': Connecting to invalid output 3 of source node dropout_2_2/cond which has 1 outputs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-58d4d6287de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_acconly_untrained.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-50c0a2d97378>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     34\u001b[0m                           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                           verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'metrics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[1;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3071\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    460\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m       \u001b[0m_initialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;31m# marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m     is_initialized = session.run(\n\u001b[1;32m--> 879\u001b[1;33m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    880\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\devmi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Node 'training/Adam/gradients/dropout_2_2/cond_grad/If': Connecting to invalid output 3 of source node dropout_2_2/cond which has 1 outputs"
     ]
    }
   ],
   "source": [
    "cnn_model = tf.keras.models.load_model('cnn_acconly_untrained.h5')\n",
    "train(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2266/2266 [==============================] - 1s 311us/sample - loss: 0.5609 - acc: 0.8694\n",
      "\n",
      "Accuracy on test data: 0.87\n",
      "\n",
      "Loss on test data: 0.56\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4892527e-04 1.9143837e-02 1.9276749e-01 7.4593164e-03 4.5205603e-04\n",
      "  3.3515282e-02 1.3556823e-01 2.1252704e-03 5.2138116e-02 1.1267266e-02\n",
      "  5.1720178e-01 2.8012501e-02]]\n",
      "TensorFlow Lite Evaluation result is 0.0\n"
     ]
    }
   ],
   "source": [
    "path = 'CNN_2.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=path)\n",
    "interpreter.get_input_details()\n",
    "\n",
    "x_test = np.array([np.genfromtxt('test.txt',delimiter = ',')]).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "    assert False\n",
    "\n",
    "MINI_BATCH_SIZE = 1\n",
    "correct_case = 0\n",
    "for i in range(len(x_test)):\n",
    "    input_index = (interpreter.get_input_details()[0]['index'])\n",
    "    interpreter.set_tensor(input_index, x_test[i * MINI_BATCH_SIZE: (i + 1) * MINI_BATCH_SIZE])\n",
    "    interpreter.invoke()\n",
    "    output_index = (interpreter.get_output_details()[0]['index'])\n",
    "    result = interpreter.get_tensor(output_index)\n",
    "    # Reset all variables so it will not pollute other inferences.\n",
    "    interpreter.reset_all_variables()\n",
    "    # Evaluate.\n",
    "    print(result)\n",
    "    prediction = np.argmax(result)\n",
    "    if prediction == np.argmax(y_test[i]):\n",
    "        correct_case += 1\n",
    "\n",
    "print('TensorFlow Lite Evaluation result is {}'.format(correct_case * 1.0 / len(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
